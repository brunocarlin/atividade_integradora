
# Atividade Integradora

Membros
* Bruno
* Henrique
* Yuri

## Set Up chuncks

```{r}
library(reticulate)
```


## Importação das principais bibliotecas que serão utilizadas

```{python}
import pandas as pd
import numpy as np
```

## Criação do dataframe utilizando o pandas como bilbioteca de manipulação de bases

```{python}
df1 = pd.read_feather("BD_PRE.feather")
```

## Describe 

```{python}
df1.describe()
```

## Realizar o drop das colunas que não serão utilizadas no modelo (IDTNS,TIPO,OPERADORA,ESTADO,DATA,H0,Q1,Q2,Q3,Q4,Q5,Q7)

### As variáveis foram retiradas por possuírem valores categóricos e únicos que não seriam interessantes para análise. Pois, sem variância destas features não agregariam em nada para o modelo.

```{python}
df1=df1.drop(["IDTNS","TIPO","DATA","H0","Q1","Q2","Q3","Q4","Q6","Q7"],axis=1)

df1.head()
```

## Renomeação da coluna J1 para target facilitando a análise da base

```{python}
df1 = df1.rename(columns = {'J1':'Target'})
```

```{python}
df1.head()
```

## Realizando a limpeza da base de acordo com o metadados disponibilizada no site do governo. Para cada variável foi feita uma manipulação de dados, no caso abaixo para toda variável em que o valor era 99 foi definida que esta seria missing. 

```{python}
df2 =  df1.copy()
```

```{python}
df2['B1_1'].replace([99], np.NaN,inplace = True)
df2['B1_2'].replace([99], np.NaN,inplace = True)
df2['C1_1'].replace([99], np.NaN,inplace = True)
df2['C1_2'].replace([99], np.NaN,inplace = True)
df2['D2_1'].replace([99], np.NaN,inplace = True)
df2['D2_2'].replace([99], np.NaN,inplace = True)
df2['D2_3'].replace([99], np.NaN,inplace = True)
df2['F5'].replace([99], np.NaN,inplace = True)
df2['F4'].replace([99], np.NaN,inplace = True)
df2['F2'].replace([99], np.NaN,inplace = True)
df2['A5'].replace([99], np.NaN,inplace = True)
df2['A4'].replace([99], np.NaN,inplace = True)
df2['A3'].replace([99], np.NaN,inplace = True)
df2['A2_1'].replace([99], np.NaN,inplace = True)
df2['A2_2'].replace([99], np.NaN,inplace = True)
df2['A2_3'].replace([99], np.NaN,inplace = True)
df2['E1_1'].replace([99], np.NaN,inplace = True)
df2['E1_2'].replace([99], np.NaN,inplace = True)
df2['E1_3'].replace([99], np.NaN,inplace = True)
df2['F4'].replace([99], np.NaN,inplace = True)
df2['F5'].replace([99], np.NaN,inplace = True)
df2['F6'].replace([99], np.NaN,inplace = True)
```

## Mesmo caso anterior, porém para os outros casos de valores considerados como missing.

```{python}
df2['Q8'].replace([999999], np.NaN,inplace = True)
df2['H1'].replace([99,99999], np.NaN,inplace = True)
df2['H2'].replace([99997,99998,99999,100000,999998,999999], np.NaN,inplace = True)
```

## A feature H2a foi removida, pois logo após seria criada um novo range de valores de salários nomeada como RIQUEZA

```{python}
df2.drop(["H2a"],inplace = True,axis = 1)
```

```{python}
df3 = df2.copy()
```

```{python}
df3.loc[(df3["H2"] >=0) & (df3["H2"] <1000), "RIQUEZA"]=1
df3.loc[(df3["H2"] >=1000) & (df3["H2"] <3000), "RIQUEZA"]=2
df3.loc[(df3["H2"] >=3000) & (df3["H2"] <6000), "RIQUEZA"]=3
df3.loc[(df3["H2"] >=6000) & (df3["H2"] <10000), "RIQUEZA"]=4
df3.loc[(df3["H2"] >=10000) & (df3["H2"] <15000), "RIQUEZA"]=5
df3.loc[(df3["H2"] >=15000) & (df3["H2"] <20000), "RIQUEZA"]=6
df3.loc[(df3["H2"] >=20000), "RIQUEZA"]=7
```

```{python}
df3.RIQUEZA.value_counts(dropna =False)
```

## Target Variable

        No código abaixo, foi definida para os valores de 99 para a Target e logo após todas as linhas que continha Target iguais a missing foram removidas da análise. Além disso, foi criada uma nova variável Target2 definindo se a nota da operadora foi RUIM (0) ou se a nota foi BOA (1).

```{python}
df3['Target'].replace([99], np.NaN,inplace = True)

df3.loc[(df3["Target"] <=8) ,"Target2"]= 0
df3.loc[(df3["Target"] >8 ) ,"Target2"]= 1


df3.dropna(subset=['Target'],inplace = True)

```


```{python}
df3.describe()
```


Variaveis Categoricas Moda
Estado  
Operadora  
RIQUEZA  
Q9  
I1 
D1     
Q5    
F1
F3  
F5  
G1

Variaveis Categoricas Missing Explicito
A1_x

## No código abaixo identificamos que existiam features em que caso o valor estivesse como missing ele representava igual a 0.

```{python}
df3["A1_1"].fillna(0,inplace = True)
df3["A1_2"].fillna(0,inplace = True)
df3["A1_3"].fillna(0,inplace = True)
df3["A1_4"].fillna(0,inplace = True)
df3["F1"].fillna(0,inplace = True)
df3["F3"].fillna(0,inplace = True)
df3["F5"].fillna(0,inplace = True)
```

## Após a correção de todos os valores da base, o código abaixo definiu no dataframe se todas as variáveis que seriam categóricas.

```{python}
df3['Q9'] = df3['Q9'].astype('category')
df3['I1'] = df3['I1'].astype('category')
df3['D1'] = df3['D1'].astype('category')
df3['Q5'] = df3['Q5'].astype('category')
df3['F1'] = df3['F1'].astype('category')
df3['F3'] = df3['F3'].astype('category')
df3['F5'] = df3['F5'].astype('category')
df3['G1'] = df3['G1'].astype('category')
df3["A1_1"] =  df3['A1_1'].astype('category')
df3["A1_2"] =  df3['A1_2'].astype('category')
df3["A1_3"] =  df3['A1_3'].astype('category')
df3["A1_4"] =  df3['A1_4'].astype('category')
df3["RIQUEZA"] =  df3['RIQUEZA'].astype('category')
df3["Target2"] =  df3['Target2'].astype('category')
```





```{python}
df3.dtypes
```

## Realizar a importação dos modelos que serão utilizados

```{python}
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
```

## Separação da base de dados em features e Target

Para as variáveis que dependiam da resposta de outras, estas foram retiradas da análise. Desta forma, foram apenas contempladas as variáveis que seja indepedentes de outras.

```{python}
df4=df3.loc[:,['Q5','Q8','Q8a','Q9','B1_1','B1_2','C1_1','C1_2','D1','D2_1','D2_2','D2_3','E1_1','E1_2','E1_3','A1_1','A1_2','A1_3','A1_4','F1','F3','F5','G1','H1','I1','PESO','RIQUEZA',"Target2"]]
```

```{r}
df <- py$df4
```

# Inicio R

```{r}
library(DataExplorer)
library(tidyverse)
library(tidymodels)
library(furrr)
```

```{r}
DataExplorer::introduce(df)
```

```{r}
DataExplorer::plot_intro(df)
```

```{r}
plot_missing(df)
```

```{r}
df <- df %>% 
  select(-starts_with("D2"))
```

```{r}
plot_missing(df)
```

```{r}
#df <- df %>% 
#  mutate(RIQUEZA = RIQUEZA %>% fct_explicit_na())
```

```{r}
  df <- df %>% 
  mutate(response = Target2 %>% fct_recode(bad = "0",good ="1")) %>% 
  select(-Target2)
```

```{r}
df %>% 
  count(response) %>%
  ggplot(aes(response, n, fill = response)) + 
  geom_col(width = .5, show.legend = FALSE) + 
  scale_y_continuous(labels = scales::comma) +
  scale_fill_manual(values = c("red","blue")) +
  labs(
    x = NULL,
    y = NULL,
    title = "Distribution of cases"
  )
  
```


# Modelagem

```{r}
telefone_initial_split <- df %>% rsample::initial_split(prop = 0.8)
telefone_initial_split
```

```{r}
train_data <- training(telefone_initial_split)
test_data <- testing(telefone_initial_split)
train_data %>% glance()
```

```{r}
recipe_telefone <- 
  recipe(response ~.,data = train_data) %>%
  step_upsample(response,skip = TRUE) %>% 
  step_normalize(all_numeric()) %>%
  step_modeimpute(all_predictors(),-all_numeric()) %>% 
  step_medianimpute(all_predictors(),-all_nominal()) %>%
  step_dummy(all_nominal(), -all_outcomes())
```



```{r}
recipe_telefone %>% prep(retain = TRUE)
```

```{r}
prepped <- recipe_telefone %>% prep(retain = TRUE) %>%  juice()
```

## Simple model

```{r}
simple_model_recipe <- recipe_telefone %>%
  prep(retain = TRUE)

simple_train <- simple_model_recipe %>% juice()

simple_test <- simple_model_recipe %>% bake(test_data)

simple_model_trained <- 
  logistic_reg(mode = "classification",penalty = 0) %>%
  set_engine("glmnet") %>% 
  fit(response ~.,data = simple_train)

simple_model_trained %>% 
  predict(simple_test) %>% 
  bind_cols(simple_test %>% select(response)) %>% 
  metrics(truth = response,estimate = .pred_class)

simple_model_trained %>% 
  predict(simple_test,type = "prob") %>% 
  bind_cols(simple_test %>% select(response)) %>% 
  roc_auc(truth = response,predictor =.pred_bad)
```

## Cross validation


### Create Cross validation dataset

```{r}
train_data_cv <- train_data %>%
  rsample::vfold_cv(v = 5)
```

## Create recipe function
```{r}

recipe_function <- function(dataset){
recipe(response ~.,data = dataset) %>%
  step_upsample(response,skip = TRUE) %>% 
  step_normalize(all_numeric()) %>%
  step_modeimpute(all_predictors(),-all_numeric()) %>% 
  step_medianimpute(all_predictors(),-all_nominal()) %>%
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  prep(data = dataset,retain = TRUE)
}
```



```{r}
cv_roc_auc_calculator <- function(cv_model,cv_dataset,cv_recipe = recipe_function) {

analysis_set <- cv_dataset %>% analysis()
analysis_prepped <- analysis_set %>% cv_recipe
analysis_juiced <- analysis_prepped %>% juice()
assessment_baked <- analysis_prepped %>% 
  bake(cv_dataset %>% assessment())

model_fit <- cv_model %>% 
  fit(response ~.,data = analysis_juiced)

roc_auc_result <- model_fit %>% 
  predict(assessment_baked,type = "prob") %>% 
  bind_cols(assessment_baked %>% select(response)) %>% 
  roc_auc(truth = response,predictor =.pred_bad) %>% 
  pull(.estimate)

return(list(model_fit = model_fit,roc_auc_result = roc_auc_result))
}
```

### Testing our function

```{r}
cv_roc_auc_calculator(cv_model = logistic_reg(mode = "classification"),
                      cv_dataset = train_data_cv$splits$`1`,
                      cv_recipe = recipe_function)
```


## Testing a bunch of models

### Lasso

```{r}
grid_lasso<- grid_random(penalty,
            mixture %>% value_set(0),
            size = 16)
grid_lasso
```


### Ridge

```{r}
grid_ridge <- grid_random(penalty,
            mixture %>% value_set(1),
            size = 16)
grid_ridge
```

```{r}
grid_elastic <- grid_random(penalty,
            mixture,
            size = 16)
grid_elastic
```

### KNN

```{r}
grid_knn <- grid_regular(neighbors %>% range_set(c(1,64))
                         ,levels = 16)
grid_knn
```

### Random Forest + Xgboost

```{r}
grid_forest <- grid_random(trees,
            mtry %>% range_set(c(1,length(df))),
            size = 16)
grid_forest
```

## Create the tibble with the models

### Models

#### Lasso grid_knn

# ```{r}
#  model_lasso <- logistic_reg(mode = "classification") %>%
#    set_engine("glmnet") %>%
#    merge(grid_lasso) %>%
#    crossing(train_data_cv) %>%
#    rename(model_spec = 1)
# 
# plan(multicore,workers = 2)
#  model_lasso_result_set <- model_lasso %>%
#    head(4) %>% 
#    mutate(roc_auc_results = future_map2(.x = model_spec,.y = splits,.f = cv_roc_auc_calculator))
# 
#  results_table <- model_lasso_result_set %>%
#    pull(roc_auc_results) %>%
#    transpose() %>%
#    as_tibble() %>%
#    mutate(roc_auc_result = roc_auc_result %>% unlist())
# 
#  models_results <- results_table %>% bind_cols(model_lasso_result_set)
# 
#  cv_results <- models_results %>%
#    group_by(model_spec %>% as.character()) %>%
#    mutate(mean_cv = mean(roc_auc_result)) %>%
#    arrange(mean_cv %>% desc) %>%
#    ungroup()
# 
#  cv_results_best <- cv_results %>% head(1)
# 
#  cv_results_best %>% pull(model_spec)
# 
#  cv_results_best %>%
#    pull(model_fit) %>%
#    pluck(1) %>%
#    pluck(2)
# 
#  cv_results_best %>% pull(mean_cv)
# 
#  cv_results_best %>%
#    pull(model_fit) %>%
#    pluck(1) %>%
#    predict(simple_test) %>%
#    bind_cols(simple_test %>% select(response)) %>%
#    metrics(truth = response,estimate = .pred_class)
# 
#  cv_results_best %>%
#    pull(model_fit) %>%
#    pluck(1) %>%
#    predict(simple_test,type = "prob") %>%
#    bind_cols(simple_test %>% select(response)) %>%
#    roc_auc(truth = response,predictor =.pred_bad)
# ```


```{r}
evaluate_model <- function(model,grid,training_data,testing_data) {
model_cross <- model %>% 
  merge(grid) %>%
  crossing(training_data) %>%
  rename(model_spec = 1)

plan(multicore, workers = 8)

model_result_set <- model_cross %>%
  mutate(roc_auc_results = future_map2(.x = model_spec,.y = splits,.f = cv_roc_auc_calculator))

results_table <- model_result_set %>%
  pull(roc_auc_results) %>%
  transpose() %>%
  as_tibble() %>% 
  mutate(roc_auc_result = roc_auc_result %>% unlist())

models_results <- results_table %>% bind_cols(model_lasso_result_set)

cv_results <- models_results %>% 
  group_by(model_spec %>% as.character()) %>% 
  mutate(mean_cv = mean(roc_auc_result)) %>%
  arrange(mean_cv %>% desc) %>% 
  ungroup()

cv_results_best <- cv_results %>% head(1)

best_fit <- cv_results_best %>% pull(model_spec)

best_cv_roc <- cv_results_best %>% pull(mean_cv)

best_metrics <- cv_results_best %>%
  pull(model_fit) %>%
  pluck(1) %>% 
  predict(simple_test) %>% 
  bind_cols(simple_test %>% select(response)) %>% 
  metrics(truth = response,estimate = .pred_class)

best_roc <- cv_results_best %>%
  pull(model_fit) %>%
  pluck(1) %>% 
  predict(simple_test,type = "prob") %>% 
  bind_cols(simple_test %>% select(response)) %>% 
  roc_auc(truth = response,predictor =.pred_bad)

return(list(best_fit = best_fit,best_cv_roc = best_cv_roc,best_metrics = best_metrics,best_roc = best_roc))
}
```

```{r}
lasso_evaluation <- evaluate_model(model = logistic_reg(mode = "classification") %>% set_engine("glmnet"),grid = grid_lasso,training_data = train_data_cv,testing_data = simple_test)
```



##### Ridge

```{r}
ridge_evaluation <- evaluate_model(model = logistic_reg(mode = "classification") %>% set_engine("glmnet"),grid = grid_ridge,training_data = train_data_cv,testing_data = simple_test)
```


##### Elastic

```{r}
elastic_evaluation <- evaluate_model(model = logistic_reg(mode = "classification") %>% set_engine("glmnet"),grid = grid_elastic,training_data = train_data_cv,testing_data = simple_test)
```

#### KNN

```{r}
knn_evaluation <- evaluate_model(model = nearest_neighbor(mode = "classification") %>% set_engine("kknn"),grid = grid_knn,training_data = train_data_cv,testing_data = simple_test)
```

##### Random Forest

```{r}
random_forest_evaluation <- evaluate_model(model = rand_forest(mode = "classification") %>% set_engine("ranger"),grid = grid_forest,training_data = train_data_cv,testing_data = simple_test)
```

#### Boosting

```{r}
boosting_evaluation <- evaluate_model(model = boost_tree(mode = "classification") %>% set_engine("xgboost"),grid = grid_forest,training_data = train_data_cv,testing_data = simple_test)
```

